# =======
# Imports
# =======

from cython import boundscheck,wraparound
from libc.stdlib cimport malloc, free
from libc.stdio cimport fflush,stdout,printf
from libc.stdlib cimport exit

from .MatrixOperations cimport CopyVector
from .MatrixOperations cimport EuclideanNorm
from .MatrixOperations cimport NormalizeVectorAndCopy
from .MatrixOperations cimport NormalizeVectorInPlace
from .MatrixOperations cimport DenseMatrixVectorMultiplication
from .MatrixOperations cimport SparseMatrixVectorMultiplication

# ============================
# Golub-Kahn Bidiagonalization
# ============================

@boundscheck(False)
@wraparound(False)
cdef int GolubKahnBidiagonalization(
        const double[:,::1] A,
        const double[:] A_Data,
        const int[:] A_ColumnIndices,
        const int[:] A_IndexPointer,
        const double[:] v,
        const int n,
        const int m,
        const double Tolerance,
        double[:] alpha,
        double[:] beta) nogil:
    """
    B-diagonalizes the positive-definite matrix ``A`` using Golub-Kahn-Lanczos method.

    This method bi-diagonalizes matrix ``A`` to ``B`` using the start vector ``w``. ``m`` is the Lanczos
    degree, which will be the size of square matrix ``B``.

    The output of this function are ``alpha`` (of length ``m``) and ``beta`` (of length ``m+1``) which 
    are diagonal (``alpha[:]``) and off-diagonal (``beta[1:]``) elements of the bi-diagonal ``(m,m)`` 
    symmetric and positive-definite matrix ``B``. 
    
    Comparision of Lanczos tri-diagonalization and Golub-Kahn Bi-diagonalization:
        * The Lanczos tri-diagonalization is twice faster (in run time), as it have only one matrix-vector
          multiplication. Whereas the Golub-Kahn bid-diagonalization has two matrix-vector multiplications.
        * The Lanczos tri-diagonalization can only be applied to symmetric matrices. Whereas the Golub-Kahn
          bi-diagonalization can be applied to any matrix.

    .. warning::

        When matrix ``A`` is very close to the identity matrix, the Golub-Kahn bi-diagonalization method can not 
        find :math:`\\beta`, as :math:`\\beta` becomes zero. If ``A`` is not exactly identity, you may decrease 
        the Tolerance to a very small number. However, if ``A`` is almost identity matrix, decreasing tolerance 
        will not help, and this function cannot be used.

    Calling this function for dense versus sparse matrices:
        * For dense matrix, call this function by:

            .. code-block:: python

                >>> # Assuming A is a memoryview to numpy.ndarray
                >>> LanczosTridiagonalization(A,None,None,None,v,n,m,Tolerance,alpha,beta)

        * For sparse matrix, call this function by:

            .. code-block:: python

                >>> # Assuming A is a scipy.sparse.csr_matrix
                >>> LanczosTridiagonalization(None,A.data,A_indices,A.indptr,v,n,m,Tolerance,alpha,beta)

    :param A: Input matrix of the size ``n*n``. Matrix should be positive-definite, but may not be symmetric.
        If ``A`` is provided (not ``None``), it is assumed that the input matrix is dense, then the arguments 
        ``A_Data``, ``A_ColumnIndices``, and ``A_IndexPointer`` should be set to ``None``.
    :type A: cython.memoryview

    :param A_Data: CSR format data array of the sparse matrix. The length of this array is the nnz of 
        the matrix. If this argument is provided (not ``None``), it is assumed that the input matrix is
        sparse. Also ``A_ColumnIndices`` and ``A_IndexPointer`` should be provided, and ``A`` should be set 
        to ``None``.
    :type A_Data: cython.memoryview

    :param A_ColumnIndices: CSR format column indices of the sparse matrix. The length of this array is the 
        nnz of the matrix. If ``A_ColumnIndices`` is provided (not ``None``), it is assumed that the input
        matrix is sparse, then, the arguments ``A_Data`` and ``A_IndexPointer`` should also be provided, and
        ``A`` should be set to ``None``.
    :type A_ColumnIndices: cython.memoryview

    :param A_IndexPointer: CSR format index pointer. The length of this array is one plus the number of 
        rows of the matrix. Also, the first element of this array is ``0``, and the last element is the nnz 
        of the matrix. If this argument is provided (not ``None``), it is assumed that the input matrix is
        sparse. Then, the arguments ``A_Data`` and ``A_ColumnIndices`` should also be provided, and ``A`` 
        should be set to ``None``.
    :type A_IndexPointer: cython.memoryview

    :param v: Start vector for the Lanczos tri-diagonalization. Column vector of size ``n``.
        It could be generated randomly. Often it is generated by the Rademacher distribution with entries 
        ``+1`` and ``-1``.
    :type v: cython.memoryview

    :param n: Size of the square matrix ``A``, which is also the size of the vector ``v``.
    :type n: int

    :param m: Lanczos degree, which is the number of Lanczos iterations.
    :type m: int

    :param Tolerance: The tolerance of the residual error of the Lanczos iteration.
    :type Tolerance: float

    :param alpha: This is a 1D array of size ``m`` and ``alpha[:]`` constitute the diagonal elements of the 
        bi-diagonal matrix ``B``. This is the output and written in place.
    :type alpha: cython.memoryview

    :param beta: This is a 1D array of size ``(m+1)``, and the elements ``beta[1:]`` constitute the off-diagonals 
        of the bi-diagonal matrix ``B``. This array is the output and written in place.
    :type beta: cython.memoryview

    :return: Counter for the Lanczos iterations. Normally, the size of the output matrix should be
        ``(m,m)``, which is the Lanczos degree. However, if the algorithm terminates early, the size
        of ``alpha`` and ``beta``, and hence the output tri-diagonal matrix, is smaller. This
        counter keeps track of the *non-zero* size of ``alpha`` and ``beta``.
    :rtype: int

    Reference:
        * NetLib `Algorithm 6.27 <http://www.netlib.org/utk/people/JackDongarra/etemplates/node198.html>`_
        * Matrix Computations, Golub, p. 495
        * Templates for Solution of Algebraic Eigenvalue Problem, Demmel, p. 143
    """

    cdef int UseSparse
    cdef int i,k
    cdef int LanczosCounter = 0

    # Check if the input matrix is sparse
    if A is None:
        if (A_Data is None) or (A_ColumnIndices is None) or (A_IndexPointer is None):
            printf('ERROR: All components of sparse matrix are not provided.\n')
            fflush(stdout)
            exit(1)
        else:
            UseSparse = 1
    else:
        UseSparse = 0

    # Allocate vectors v_old, v_new, u_old, and u_new
    cdef double* v_old = <double*> malloc(sizeof(double)*n)
    cdef double* v_new = <double*> malloc(sizeof(double)*n)
    cdef double* u_old = <double*> malloc(sizeof(double)*n)
    cdef double* u_new = <double*> malloc(sizeof(double)*n)

    # Normalize vector v and copy to v_old
    NormalizeVectorAndCopy(&v[0],n,v_old)

    # Initialize beta[0]
    beta[0] = 0.0

    # Golub-Kahn iteration
    for k in range(m):

        # Counter for the non-zero size of alpha and beta
        LanczosCounter += 1

        # u_new = A.dot(v_old)
        if UseSparse:
            # Sparse matrix-vector multiplication
            SparseMatrixVectorMultiplication(A_Data,A_ColumnIndices,A_IndexPointer,v_old,n,u_new)
        else:
            # Dense matrix-vector multiplication
            DenseMatrixVectorMultiplication(A,v_old,n,n,u_new)

        if k > 0:
            for i in range(n):
                u_new[i] = u_new[i] - beta[k]*u_old[i]

        # Normalize u_new and set its norm to alpha[k]
        alpha[k] = NormalizeVectorInPlace(u_new,n)

        # v_new is A.u_new - alpha[k].v_old
        if UseSparse:
            # Sparse matrix-vector multiplication
            SparseMatrixVectorMultiplication(A_Data,A_ColumnIndices,A_IndexPointer,u_new,n,v_new)
        else:
            # Dense matrix-vector multiplication
            DenseMatrixVectorMultiplication(A,u_new,n,n,v_new)

        for i in range(n):
            v_new[i] = v_new[i] - alpha[k]*v_old[i]

        # Update beta as the norm of v_new
        beta[k+1] = EuclideanNorm(v_new,n)

        # Exit criterion
        if beta[k+1] < Tolerance:
            if k == 0:
                printf('Premature exit in Golub-Kahn-Lanczos bi-diagonalization. Probably A is close to identity. alpha[0]: %e, beta[1]: %e, Tolerance: %e',alpha[0],beta[1],Tolerance)
                fflush(stdout)
            break

        # Scale v_new by 1/beta[k+1]
        for i in range(n):
            v_new[i] = v_new[i] / beta[k+1]

        # Update for new iteration
        CopyVector(v_new,n,v_old)
        CopyVector(u_new,n,u_old)

    # Free dynamic memory
    free(v_old)
    free(v_new)
    free(u_old)
    free(u_new)

    return LanczosCounter
