# =======
# Imports
# =======

from cython import boundscheck,wraparound
from libc.stdlib cimport malloc,free
from libc.stdio cimport fflush,stdout,printf
from libc.stdlib cimport exit

from .MatrixOperations cimport CopyVector
from .MatrixOperations cimport InnerProduct
from .MatrixOperations cimport EuclideanNorm
from .MatrixOperations cimport DenseMatrixVectorMultiplication
from .MatrixOperations cimport SparseMatrixVectorMultiplication

# ==========================
# Lanczos Tridiagonalization
# ==========================

@boundscheck(False)
@wraparound(False)
cdef int LanczosTridiagonalization(
        const double[:,::1] A,
        const double[:] A_Data,
        const int[:] A_ColumnIndices,
        const int[:] A_IndexPointer,
        const double[:] v,
        const int n,
        const int m,
        const double Tolerance,
        double[:] alpha,
        double[:] beta) nogil:
    """
    Tri-diagonalizes matrix ``A`` to ``T`` using the start vector ``v``. ``m`` is the Lanczos
    degree, which will be the size of square matrix ``T``.

    The output of this function are ``alpha`` (of length ``m``) and ``beta`` (of length ``m+1``) which 
    are diagonal (``alpha[:]``) and off-diagonal (``beta[1:]``) elements of the tri-diagonal ``(m,m)`` 
    symmetric and positive-definite matrix ``T``. 

    Comparison of Lanczos tri-diagonalization and Golub-Kahn Bi-diagonalization:
        * The Lanczos tri-diagonalization is twice faster (in run time), as it have only one matrix-vector
          multiplication. Whereas the Golub-Kahn bid-diagonalization has two matrix-vector multiplications.
        * The Lanczos tri-diagonalization can only be applied to symmetric matrices. Whereas the Golub-Kahn
          bi-diagonalization can be applied to any matrix.

    Calling this function for dense versus sparse matrices:
        * For dense matrix, call this function by:

            .. code-block:: python

                >>> # Assuming A is a memoryview to numpy.ndarray
                >>> LanczosTridiagonalization(A,None,None,None,v,n,m,Tolerance,alpha,beta)

        * For sparse matrix, call this function by:

            .. code-block:: python

                >>> # Assuming A is a scipy.sparse.csr_matrix
                >>> LanczosTridiagonalization(None,A.data,A_indices,A.indptr,v,n,m,Tolerance,alpha,beta)

    :param A: Input matrix of the size ``n*n``. Matrix should be positive-definite and symmetric.
        If ``A`` is provided (not ``None``), it is assumed that the input matrix is dense, then the arguments 
        ``A_Data``, ``A_ColumnIndices``, and ``A_IndexPointer`` should be set to ``None``.
    :type A: cython.memoryview

    :param A_Data: CSR format data array of the sparse matrix. The length of this array is the nnz of 
        the matrix. If this argument is provided (not ``None``), it is assumed that the input matrix is
        sparse. Also ``A_ColumnIndices`` and ``A_IndexPointer`` should be provided, and ``A`` should be set 
        to ``None``.
    :type A_Data: cython.memoryview

    :param A_ColumnIndices: CSR format column indices of the sparse matrix. The length of this array is the 
        nnz of the matrix. If ``A_ColumnIndices`` is provided (not ``None``), it is assumed that the input
        matrix is sparse, then, the arguments ``A_Data`` and ``A_IndexPointer`` should also be provided, and
        ``A`` should be set to ``None``.
    :type A_ColumnIndices: cython.memoryview

    :param A_IndexPointer: CSR format index pointer. The length of this array is one plus the number of 
        rows of the matrix. Also, the first element of this array is ``0``, and the last element is the nnz 
        of the matrix. If this argument is provided (not ``None``), it is assumed that the input matrix is
        sparse. Then, the arguments ``A_Data`` and ``A_ColumnIndices`` should also be provided, and ``A`` 
        should be set to ``None``.
    :type A_IndexPointer: cython.memoryview

    :param v: Start vector for the Lanczos tri-diagonalization. Column vector of size ``n``.
        It could be generated randomly. Often it is generated by the Rademacher distribution with entries 
        ``+1`` and ``-1``.
    :type v: cython.memoryview

    :param n: Size of the square matrix ``A``, which is also the size of the vector ``v``.
    :type n: int

    :param m: Lanczos degree, which is the number of Lanczos iterations.
    :type m: int

    :param Tolerance: The tolerance of the residual error of the Lanczos iteration.
    :type Tolerance: float

    :param alpha: This is a 1D array of size ``m`` and ``alpha[:]`` constitute the diagonal elements of the 
        tri-diagonal matrix ``T``. This is the output and written in place.
    :type alpha: cython.memoryview

    :param beta: This is a 1D array of size ``(m+1)``, and the elements ``beta[1:]`` constitute the off-diagonals 
        of the tri-diagonal matrix ``T``. This array is the output and written in place.
    :type beta: cython.memoryview

    :return: Counter for the Lanczos iterations. Normally, the size of the output matrix should be
        ``(m,m)``, which is the Lanczos degree. However, if the algorithm terminates early, the size
        of ``alpha`` and ``beta``, and hence the output tri-diagonal matrix, is smaller. This
        counter keeps track of the *non-zero* size of ``alpha`` and ``beta``.
    :rtype: int

    Reference:
        * Templates for solution of Algebraic Eigenvalue Problems, James Demmel, p.57
    """

    cdef int UseSparse
    cdef int i,j,k
    cdef int LanczosCounter = 0

    # Check if the input matrix is sparse
    if A is None:
        if (A_Data is None) or (A_ColumnIndices is None) or (A_IndexPointer is None):
            printf('ERROR: All components of sparse matrix are not provided.\n')
            fflush(stdout)
            exit(1)
        else:
            UseSparse = 1
    else:
        UseSparse = 0

    # Allocate 2D array (as 1D array, and coalesced row-wise) to hold orthogonalized vectors
    cdef double* V = <double*> malloc(sizeof(double)*n*m)

    # Allocate vector r
    cdef double* r = <double*> malloc(sizeof(double)*n)

    # Copy v into r
    CopyVector(&v[0],n,r)

    # Initialize beta[0]
    beta[0] = EuclideanNorm(r,n)

    # In the following, beta[j] means beta[j-1] in the Demmel text
    for j in range(m):

        # Update counter
        LanczosCounter += 1

        # Normalize r and save to the j-th column of V
        for i in range(n):
            V[j*n+i] = r[i] / beta[j]

        # Multiply A to the j-th column of V, written into r
        if UseSparse:
            # Sparse matrix-vector multiplication
            SparseMatrixVectorMultiplication(A_Data,A_ColumnIndices,A_IndexPointer,&V[j*n],n,r)
        else:
            # Dense matrix-vector multiplication
            DenseMatrixVectorMultiplication(A,&V[j*n],n,n,r)

        if j > 0:
            # Subtract from r the V[:,j-1]*beta[j]
            for i in range(n):
                r[i] = r[i] - V[(j-1)*n+i]*beta[j]

        # alpha[j] is V[:,j] dot r
        alpha[j] = InnerProduct(&V[j*n],r,n)

        # Subtract from r the vector V[:,j]*alpha[j]
        for i in range(n):
            r[i] = r[i] - V[j*n+i]*alpha[j]

        # beta is norm of r
        beta[j+1] = EuclideanNorm(r,n)

        # Exit if beta got very small
        if beta[j+1]*(j+1) < Tolerance:
            break
    
    # Free dynamic memory
    free(V)
    free(r)

    return LanczosCounter
